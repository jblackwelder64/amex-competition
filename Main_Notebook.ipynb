{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f53a6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before running the notebook, open two terminal windows in your environment containing Dask, and then run\n",
    "'dask-scheduler' in one of them, and 'dask-worker <localhost address from the output of dask-scheduler>'\n",
    "in the other. \n",
    "\"\"\"\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client('tcp://127.0.0.1:8786')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58e680c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a method to prepare out dataframe for training. Note that df is intended to be a Dask dataframe.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "global column_sets\n",
    "\n",
    "def apply_restore_nan(df, column_sets):\n",
    "    return df.apply(restore_nan_category, column_sets=column_sets, axis=1)\n",
    "\n",
    "def restore_nan_category(series, column_sets):\n",
    "#     print(series, flush=True)\n",
    "    for column_set in column_sets:\n",
    "        all_zero = True\n",
    "        for col in column_set:\n",
    "#             print(series)\n",
    "            if not series[col] == 0:\n",
    "                all_zero = False\n",
    "        if all_zero:\n",
    "            for col in column_set:\n",
    "                series[col] = float('NaN')\n",
    "#     del(column_sets)\n",
    "#     gc.collect()\n",
    "    return series \n",
    "        \n",
    "\n",
    "def prepare_data(dfl0, test_time):\n",
    "    \n",
    "\n",
    "    cat_vars = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    cat_data0 = dfl0[['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', \n",
    "                               'D_64', 'D_66', 'D_68']].copy()\n",
    "    \n",
    "    dfl1 = dfl0.categorize(columns=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', \n",
    "                                            'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "    \n",
    "    dfl2 = dd.get_dummies(dfl1, drop_first=False, \n",
    "                               columns=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', \n",
    "                                        'D_64','D_66', 'D_68'])\n",
    "    \n",
    "    \n",
    "    column_sets = []\n",
    "    column_sets.append([col for col in dfl2.columns if 'B_30_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'B_38_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_114_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_116_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_117_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_120_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_126_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_63_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_64_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_66_' in col])\n",
    "    column_sets.append([col for col in dfl2.columns if 'D_68_' in col])\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    To ensure that data_onehot will have the desired shape when we use this function at test time.\n",
    "    For now, our model simply ignores categorical data, which 'if test_time' simply results in a pass.\n",
    "    \"\"\"\n",
    "#     https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data\n",
    "    if test_time:\n",
    "        pass\n",
    "#         _,data_onehot = train_data_onehot.align(df_onehot, join='outer', axis=1, fill_value=0)\n",
    "        #have to fix this df = df_onehot.reindex(columns = train_data_onehot.columns, fill_value=float('NaN'))\n",
    "    else:\n",
    "        dfl3 = dfl2\n",
    "        \n",
    "    #Assigning NaN values back to the columns representing the categorical variables that had NaN originally\n",
    "    dfl4 = dfl3.map_partitions(apply_restore_nan, column_sets=column_sets)\n",
    "    \n",
    "    # Setting the index to 'customer_ID' will help us do the following calculations\n",
    "    dfl5 = dfl4.set_index('customer_ID')\n",
    "    num_columns = []\n",
    "    cat_columns = ['B_30_', 'B_38_', 'D_114_', 'D_116_', 'D_117_', 'D_120_', \n",
    "                   'D_126_', 'D_63_', 'D_64_', 'D_66_', 'D_68_']\n",
    "    for key in list(dfl5.columns):\n",
    "        if key[0:5] in [col[0:5] for col in cat_columns]:\n",
    "            pass\n",
    "        else:\n",
    "            num_columns.append(key)\n",
    "    num_columns.remove('S_2')  #Datetime; will have to remove this line once model becomes more sophisticated\n",
    "\n",
    "    valtype_dict = {}\n",
    "    for key in num_columns:\n",
    "        if not key == 'customer_ID' and not key == 'S_2':\n",
    "            valtype_dict[key] = 'mean'\n",
    "\n",
    "    #removing all categorical columns (obviously we want to change this soon)\n",
    "    dfl6 = dfl5[num_columns]\n",
    "\n",
    "    dfl7 = dfl6.astype(float)\n",
    "    \n",
    "    #Generating some simple features by inserting the average value in each column for each customer\n",
    "    dfl8 = dfl7.groupby(\"customer_ID\").agg(valtype_dict)\n",
    "\n",
    "    return dfl8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9b87112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "def train_xgboost(train_data_onehot, train_labels, n_boost_rounds, client):\n",
    "\n",
    "    dtrain = xgb.dask.DaskDMatrix(client, train_data_onehot, train_labels)\n",
    "\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        {\"verbosity\": 2, \"tree_method\": \"hist\", \"objective\": \"reg:squarederror\"},\n",
    "        dtrain,\n",
    "        num_boost_round=n_boost_rounds,\n",
    "        evals=[(dtrain, \"train\")],\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "146d38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uncomment here and comment out the 'train_data == dd.read_csv' line below if \n",
    "testing with 10,000 line subset of training data.\n",
    "\"\"\"\n",
    "\n",
    "train_data = pd.read_csv('./train_data.csv', nrows=10000)\n",
    "train_data = dd.from_pandas(train_data, npartitions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6d71bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here 5\n",
      "here 6\n",
      "here 7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now trying to repeat the above with a full sized training data set.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Uncomment below if using the full training dataset\n",
    "# train_data = dd.read_csv('./train_data.csv', blocksize=25e6)\n",
    "train_data2 = train_data\n",
    "train_labels = pd.read_csv('./train_labels.csv', index_col='customer_ID')\n",
    "\n",
    "train_data_prepared = prepare_data(train_data2, False)\n",
    "\n",
    "\n",
    "# We do the merge below in order to ensure that the partitions of the dask dataframes line up\n",
    "train_data_onehot_and_labels = dd.merge(train_data_prepared, train_labels) #, left_index=True, right_index=True)\n",
    "X_train = train_data_onehot_and_labels.drop(columns=['target'])\n",
    "y_train = train_data_onehot_and_labels['target']\n",
    "output = train_xgboost(X_train, y_train, 20, client) #third arg is number of training iterations I think\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd0e1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump( output, open( \"./Models/model_trained_on_10k_subset.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a1b216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xgboost.core.Booster'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ToDo: Now we start writing code to load the model trained above, and then ultimately output a submission .csv file\n",
    "containing our predictions over the test dataset.\n",
    "\"\"\"\n",
    "\n",
    "file = open('./Models/model2.p', 'rb')\n",
    "model_output = pickle.load(file)\n",
    "print(type(model_output['booster']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amex_env]",
   "language": "python",
   "name": "conda-env-amex_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
