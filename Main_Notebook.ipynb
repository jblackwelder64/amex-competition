{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53a6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before running the notebook, open two terminal windows in your environment containing Dask, and then run\n",
    "'dask-scheduler' in one of them, and 'dask-worker <localhost address from the output of dask-scheduler>'\n",
    "in the other. \n",
    "\"\"\"\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client('tcp://127.0.0.1:8786')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f7b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to convert and store our train and test CSV data files into parquet. Cell only needs to run once. This will make\n",
    "later tasks much more efficient than reading from CSV file.\n",
    "\"\"\"\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# train_data = dd.read_csv('./train_data.csv')\n",
    "# dd.to_parquet(train_data, './Train-Parquet', overwrite=True)\n",
    "\n",
    "# test_data = dd.read_csv('./test_data.csv')\n",
    "# dd.to_parquet(test_data, './Test-Parquet', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e680c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a method to prepare our dataframe for training. Note that df is intended to be a Dask dataframe.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# global column_sets\n",
    "# global train_cat_columns_global\n",
    "\n",
    "\n",
    "def apply_restore_nan(df, column_sets):\n",
    "    return df.apply(restore_nan_category, column_sets=column_sets, axis=1)\n",
    "\n",
    "def restore_nan_category(series, column_sets):\n",
    "#     print(series, flush=True)\n",
    "    for column_set in column_sets:\n",
    "        all_zero = True\n",
    "        for col in column_set:\n",
    "#             print(series)\n",
    "            if not series[col] == 0:\n",
    "                all_zero = False\n",
    "        if all_zero:\n",
    "            for col in column_set:\n",
    "                series[col] = float('NaN')\n",
    "#     del(column_sets)\n",
    "#     gc.collect()\n",
    "    return series \n",
    "\n",
    "def custom_mode(x):\n",
    "#     print(x)\n",
    "    if not len(x) == len(x[x.isna()]):\n",
    "#         print('X DOESNT ONLY CONTAIN NANS!')\n",
    "        x1 = x[~x.isna()]\n",
    "        return pd.Series.mode(x1,dropna=False)[0]\n",
    "    else:\n",
    "        return float('NaN')\n",
    "    \n",
    "        \n",
    "def prepare_data(dfl0, test_time):\n",
    "    \n",
    "\n",
    "    print('Doing CAT stuff')\n",
    "    \"\"\"\n",
    "    Preparing the categorical variables separately\n",
    "    \"\"\"\n",
    "    \n",
    "    cat_vars = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    cat_data0 = dfl0[['customer_ID','B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', \n",
    "                               'D_64', 'D_66', 'D_68']].compute()\n",
    "    \n",
    "#     cat_data1 = cat_data0.categorize(columns=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', \n",
    "#                                             'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "    \n",
    "\n",
    "    num_columns = []\n",
    "    for key in list(dfl0.columns):\n",
    "        if key[0:5] in [col[0:5] for col in cat_vars]:\n",
    "            pass\n",
    "        else:\n",
    "            num_columns.append(key)\n",
    "    num_columns.remove('S_2')  #Datetime; will have to remove this line once model becomes more sophisticated\n",
    "    \n",
    "    valtype_dict_cat = {}\n",
    "    for key in cat_vars:\n",
    "        if not key == 'customer_ID' and not key == 'S_2':\n",
    "            valtype_dict_cat[key] = [custom_mode, 'last']\n",
    "    \n",
    "    \n",
    "    print(cat_data0.columns)\n",
    "    print(valtype_dict_cat)\n",
    "    cat_data_grouped = cat_data0.groupby(by='customer_ID').aggregate(valtype_dict_cat)\n",
    "    \n",
    "    cat_data_grouped_dummies = dd.get_dummies(cat_data_grouped, drop_first=False, \n",
    "                               columns=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', \n",
    "                                        'D_64','D_66', 'D_68'])\n",
    "    \n",
    "    \n",
    "    column_sets = []\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'B_30_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'B_38_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_114_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_116_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_117_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_120_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_126_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_63_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_64_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_66_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_68_' in col])\n",
    "\n",
    "#     print(cat_data_grouped.head())\n",
    "\n",
    "    \"\"\"\n",
    "    To ensure that data_onehot will have the desired shape when we use this function at test time.\n",
    "    https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data\n",
    "    \"\"\"\n",
    "    if not test_time:\n",
    "        pickle.dump( cat_data_grouped_dummies.columns, open( \"./Train-Cat-Columns/train_cat_columns.p\", \"wb\" ) )\n",
    "        cat_data_almost_final = cat_data_grouped_dummies\n",
    "#         train_cat_columns_global = cat_data_grouped_dummies.columns\n",
    "    else:\n",
    "        train_cat_columns = pickle.load(open('./Train-Cat-Columns/train_cat_columns.p', 'rb'))\n",
    "        cat_data_almost_final = cat_data_grouped_dummies.reindex(columns = train_cat_columns, fill_value=0)\n",
    "        \n",
    "    cat_data_final = cat_data_almost_final.apply(restore_nan_category, column_sets=column_sets, axis=1)\n",
    "    \n",
    "    print('DONE WITH ONLY DOING CAT STUFF')\n",
    "    \"\"\"\n",
    "    Now preparing the numerical variables\n",
    "    \"\"\"\n",
    "    dfl1 = dfl0[num_columns] #.repartition(npartitions=100*dfl0.npartitions)\n",
    "#     print(finished repartition)\n",
    "    valtype_dict_num = {}\n",
    "    for key in dfl1.columns:\n",
    "        if not key == 'customer_ID' and not key == 'S_2':\n",
    "            if key in num_columns:\n",
    "                if key == 'P_2':\n",
    "                    # size corresponds to number of rows for given customer_ID; we only need to add this once\n",
    "                    valtype_dict_num[key] = ['mean', 'std', 'min', 'max', 'size', 'last']\n",
    "                else:\n",
    "                    valtype_dict_num[key] = ['mean', 'std', 'min', 'max', 'last']\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "#     print(valtype_dict)\n",
    "        \n",
    "    dfl1_num_columns = dfl1\n",
    "    # Added the split_out parameter because I'm getting worker killed errors; should probably lower this to ~2\n",
    "    dfl1_num_columns_grouped = dfl1_num_columns.groupby(by=\"customer_ID\").aggregate(valtype_dict_num, split_out=10)\n",
    "    print('finished aggregate on num data')\n",
    "    \"\"\"\n",
    "    Joining the numerical and categorical variables back together\n",
    "    \"\"\"\n",
    "    \n",
    "    final_df = dfl1_num_columns_grouped.merge(cat_data_final, how=\"left\", on=['customer_ID'])\n",
    "    print('finished merge')\n",
    "    \n",
    "#     print(dfl1_num_columns_grouped.head(50))\n",
    "#     print(dfl1_num_columns_grouped.columns)\n",
    "#     print(dfl1_num_columns_grouped['B_30'].head(50))\n",
    "#     dfl1_2 = dfl1_grouped.categorize(columns=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', \n",
    "#                                             'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     column_sets = []\n",
    "#     column_sets.append([col for col in dfl2.columns if 'B_30_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'B_38_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_114_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_116_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_117_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_120_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_126_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_63_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_64_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_66_' in col])\n",
    "#     column_sets.append([col for col in dfl2.columns if 'D_68_' in col])\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    To ensure that data_onehot will have the desired shape when we use this function at test time.\n",
    "    For now, our model simply ignores categorical data, which 'if test_time' simply results in a pass.\n",
    "    \"\"\"\n",
    "#     https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data\n",
    "#     if test_time:\n",
    "#         dfl3 = dfl2\n",
    "# #         _,data_onehot = train_data_onehot.align(df_onehot, join='outer', axis=1, fill_value=0)\n",
    "#         #have to fix this df = df_onehot.reindex(columns = train_data_onehot.columns, fill_value=float('NaN'))\n",
    "#     else:\n",
    "#         dfl3 = dfl2\n",
    "        \n",
    "    #Assigning NaN values back to the columns representing the categorical variables that had NaN originally\n",
    "#     dfl4 = dfl3.map_partitions(apply_restore_nan, column_sets=column_sets)\n",
    "    \n",
    "    # Setting the index to 'customer_ID' will help us do the following calculations\n",
    "    \n",
    "#     dfl5 = dfl4.set_index('customer_ID')\n",
    "#     num_columns = []\n",
    "#     cat_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', \n",
    "#                    'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "#     for key in list(dfl5.columns):\n",
    "#         if key[0:5] in [col[0:5] for col in cat_columns]:\n",
    "#             pass\n",
    "#         else:\n",
    "#             num_columns.append(key)\n",
    "#     num_columns.remove('S_2')  #Datetime; will have to remove this line once model becomes more sophisticated\n",
    "\n",
    "#     valtype_dict = {}\n",
    "#     for key in dfl5.columns:\n",
    "#         if not key == 'customer_ID' and not key == 'S_2':\n",
    "#             if key in num_columns:\n",
    "#                 valtype_dict[key] = 'mean'\n",
    "#             else:\n",
    "#                 valtype_dict[key] = stats.mode\n",
    "        \n",
    "\n",
    "    #removing all categorical columns (obviously we want to change this soon)\n",
    "#     dfl6 = dfl5[num_columns]\n",
    "\n",
    "#     dfl7 = dfl6.astype(float)\n",
    "    \n",
    "    #Generating some simple features by inserting the average value in each column for each customer\n",
    "#     dfl8 = dfl7.groupby(\"customer_ID\").agg(valtype_dict)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b87112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Wrapper function to train the model. Uses https://xgboost.readthedocs.io/en/stable/tutorials/dask.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "def train_xgboost(train_data_onehot, train_labels, n_boost_rounds, client):\n",
    "\n",
    "    dtrain = xgb.dask.DaskDMatrix(client, train_data_onehot, train_labels)\n",
    "\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        {\"verbosity\": 2, \"tree_method\": \"hist\", \"objective\": \"reg:logistic\"},\n",
    "        dtrain,\n",
    "        num_boost_round=n_boost_rounds,\n",
    "        evals=[(dtrain, \"train\")],\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146d38be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nUncomment here and comment out the 'train_data == dd.read_csv' line below if \\ntesting with 10,000 line subset of training data.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Uncomment here and comment out the 'train_data == dd.read_csv' line below if \n",
    "testing with 10,000 line subset of training data.\n",
    "\"\"\"\n",
    "\n",
    "# train_data = pd.read_csv('./train_data.csv', nrows=10000)\n",
    "# train_data = dd.from_pandas(train_data, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24b9f33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing CAT stuff\n",
      "Index(['customer_ID', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
      "       'D_126', 'D_63', 'D_64', 'D_66', 'D_68'],\n",
      "      dtype='object')\n",
      "{'B_30': [<function custom_mode at 0x16352d3f0>, 'last'], 'B_38': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_114': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_116': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_117': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_120': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_126': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_63': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_64': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_66': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_68': [<function custom_mode at 0x16352d3f0>, 'last']}\n",
      "DONE WITH ONLY DOING CAT STUFF\n",
      "finished aggregate on num data\n",
      "finished merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/dask/dataframe/multi.py:399: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  meta = left._meta_nonempty.merge(right._meta_nonempty, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Processing the data and training the model\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Uncomment below if using the full training dataset. Additionally, experiment with increasing blocksize above 25e6.\n",
    "# train_data = dd.read_csv('./train_data.csv') #, blocksize=100e6)\n",
    "train_data = dd.read_parquet('./Train-Parquet')\n",
    "train_data2 = train_data\n",
    "train_labels = pd.read_csv('./train_labels.csv')\n",
    "\n",
    "train_data_prepared = prepare_data(train_data2, False)\n",
    "\n",
    "\n",
    "# We do the merge below in order to ensure that the partitions of the dask dataframes line up\n",
    "train_data_onehot_and_labels = dd.merge(train_data_prepared, train_labels, on=['customer_ID'])\n",
    "X_train = train_data_onehot_and_labels.drop(columns=['target','customer_ID'])\n",
    "y_train = train_data_onehot_and_labels['target']\n",
    "output = train_xgboost(X_train, y_train, 100, client) #third arg is number of training iterations I think\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37caf9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump( output, open( \"./Models/mean_std_min_max_last_2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe03402",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Doing CAT stuff\n",
      "Index(['customer_ID', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
      "       'D_126', 'D_63', 'D_64', 'D_66', 'D_68'],\n",
      "      dtype='object')\n",
      "{'B_30': [<function custom_mode at 0x16352d3f0>, 'last'], 'B_38': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_114': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_116': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_117': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_120': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_126': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_63': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_64': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_66': [<function custom_mode at 0x16352d3f0>, 'last'], 'D_68': [<function custom_mode at 0x16352d3f0>, 'last']}\n",
      "DONE WITH ONLY DOING CAT STUFF\n",
      "finished aggregate on num data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/dask/dataframe/multi.py:399: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  meta = left._meta_nonempty.merge(right._meta_nonempty, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished merge\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ToDo: Now we start writing code to load the model trained above, and then ultimately output a submission .csv file\n",
    "containing our predictions over the test dataset.\n",
    "\"\"\"\n",
    "import dask.dataframe as dd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_path = './Models/mean_std_min_max_last_2.p' \n",
    "file = open(model_path, 'rb')\n",
    "model_output = pickle.load(file)\n",
    "\n",
    "# test_data = dd.read_csv('./test_data.csv')#, blocksize=100e6)\n",
    "test_data = dd.read_parquet('./Test-Parquet', split_row_groups=1000)\n",
    "# customers = pd.read_parquet('./Test-Parquet', columns=['customer_ID']).to_numpy()\n",
    "print(1)\n",
    "prepared_test_data = prepare_data(test_data, True)\n",
    "customers = prepared_test_data.index\n",
    "print(2)\n",
    "\n",
    "dtest = xgb.dask.DaskDMatrix(client, prepared_test_data)\n",
    "prediction = xgb.dask.predict(client, model_output['booster'], dtest)\n",
    "print(3)\n",
    "\n",
    "output_df = pd.DataFrame({'customer_ID' : list(customers), 'prediction' : np.array(prediction)})\n",
    "print(4)\n",
    "output_df.to_csv('./Outputs/submission4.csv', index=False)\n",
    "\n",
    "# prepared_test_data1 = prepared_test_data\n",
    "# prepared_test_data['customer_ID'] = prepared_test_data.index  #.compute()\n",
    "# test_results = prepared_test_data[['customer_ID']].compute()\n",
    "# test_results['prediction'] = prediction\n",
    "# print(test_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71ff731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(prediction))\n",
    "# print(customers)\n",
    "# customers_list = []\n",
    "# for row in customers:\n",
    "#     if not row[0] in customers_list:\n",
    "#         customers_list.append(row[0])\n",
    "        \n",
    "# This is merely a stopgap function until I figure out how to really do this correctly\n",
    "# customers_list = []\n",
    "# customers_list.append(customers[0][0])\n",
    "# for i in range(1, len(customers)):\n",
    "#     if not customers[i] == customers[i-1]:\n",
    "#         customers_list.append(customers[i][0])\n",
    "    \n",
    "# print(customers_list[0:100])\n",
    "# print(len(customers_list), len(prediction))\n",
    "\n",
    "# customers = np.array(pd.read_parquet('./Test-Parquet', columns=['customer_ID']).groupby('customer_ID').count().index)\n",
    "# print(customers)\n",
    "\n",
    "# customers1 = customers.to_numpy()\n",
    "# # This is merely a stopgap function until I figure out how to really do this correctly\n",
    "# customers_list = []\n",
    "# customers_list.append(customers[0][0])\n",
    "# for i in range(1, len(customers)):\n",
    "#     if not customers[i] == customers[i-1]:\n",
    "#         customers_list.append(customers[i][0])\n",
    "    \n",
    "# # print(customers_list[0:100])\n",
    "# print(len(customers_list), len(prediction))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output_df = pd.DataFrame({'customer_ID' : customers, 'prediction' : np.array(prediction)})\n",
    "# # print(4)\n",
    "# output_df.to_csv('./Outputs/submission1.csv', index=False)\n",
    "# # # print(prepared_test_data.index.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0221015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2f5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b89e3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSanity check\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sanity check\n",
    "\"\"\"\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# prediction = np.array(prediction)\n",
    "# for i in range(10):\n",
    "#     print(prediction[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ae91f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prepared_test_data.columns)\n",
    "\n",
    "# prepared_test_data1 = prepared_test_data.copy()\n",
    "# prepared_test_data1['customer_ID'] = prepared_test_data.index  #.compute()\n",
    "# test_results = prepared_test_data1[['customer_ID']].compute()\n",
    "# test_results['prediction'] = prediction\n",
    "# print(test_results.head())\n",
    "# # print(len(test_data))\n",
    "# # print(len(test_results))\n",
    "# # test_results['prediction'] = prediction\n",
    "# # print(test_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4f840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial = [1, 1, 9, 1, 9, 6, 9, 7]\n",
    "# result = sorted(set(initial), key=initial.index)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e25045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11363762\n"
     ]
    }
   ],
   "source": [
    "# print(train_data[['customer_ID','B_30']].groupby('customer_ID').agg({'B_30':custom_mode}))\n",
    "# print(prepared_test_data.head(50))\n",
    "# print(prepared_test_data.columns)\n",
    "# print(train_data_onehot_and_labels.columns)\n",
    "# customers =list(prepared_test_data.index)\n",
    "# print(customers[0:100])\n",
    "\n",
    "# X_train.head(200).to_csv('./X_train_head')\n",
    "# train_data.head(2600).to_csv('./train_data_head')\n",
    "# train_data_prepared.head(200).to_csv('./mean_std_etc')\n",
    "# print(train_data_prepared.head().index)\n",
    "# print(train_data_prepared.columns)\n",
    "# print(train_data_prepared.loc['0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a'][('D_49','last')].compute())\n",
    "\n",
    "print(len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amex_env]",
   "language": "python",
   "name": "conda-env-amex_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
