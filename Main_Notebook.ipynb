{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53a6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before running the notebook, open two terminal windows in your environment containing Dask, and then run\n",
    "'dask-scheduler' in one of them, and 'dask-worker <localhost address from the output of dask-scheduler>'\n",
    "in the other. \n",
    "\"\"\"\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client('tcp://127.0.0.1:8786')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f7b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to convert and store our train, test CSV data files into parquet. Cell only needs to run once; uncomment below. \n",
    "This will make later tasks much more efficient (hopefully).\n",
    "\"\"\"\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# train_data = dd.read_csv('./train_data.csv')\n",
    "# dd.to_parquet(train_data, './Train-Parquet', overwrite=True)\n",
    "\n",
    "# test_data = dd.read_csv('./test_data.csv')\n",
    "# dd.to_parquet(test_data, './Test-Parquet', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e680c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a method to prepare our dataframe for training. Note that df is intended to be a Dask dataframe.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "def apply_restore_nan(df, column_sets):\n",
    "    return df.apply(restore_nan_category, column_sets=column_sets, axis=1)\n",
    "\n",
    "def restore_nan_category(series, column_sets):\n",
    "    for column_set in column_sets:\n",
    "        all_zero = True\n",
    "        for col in column_set:\n",
    "            if not series[col] == 0:\n",
    "                all_zero = False\n",
    "        if all_zero:\n",
    "            for col in column_set:\n",
    "                series[col] = float('NaN')\n",
    "    return series \n",
    "\n",
    "# mode function written to obtain the most common value of a cat variable on each group.\n",
    "def custom_mode(x):\n",
    "    if not len(x) == len(x[x.isna()]):\n",
    "        x1 = x[~x.isna()]\n",
    "        return pd.Series.mode(x1,dropna=False)[0]\n",
    "    else:\n",
    "        return float('NaN')\n",
    "    \n",
    "        \n",
    "def prepare_data(dfl0, test_time):\n",
    "    \n",
    "\n",
    "    print('Doing CAT stuff')\n",
    "    \"\"\"\n",
    "    Preparing the categorical variables separately\n",
    "    \"\"\"\n",
    "    \n",
    "    cat_vars = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    cat_data0 = dfl0[['customer_ID','B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', \n",
    "                               'D_64', 'D_66', 'D_68']] #.compute()\n",
    "    \n",
    "\n",
    "    num_columns = []\n",
    "    for key in list(dfl0.columns):\n",
    "        if key[0:5] in [col[0:5] for col in cat_vars]:\n",
    "            pass\n",
    "        else:\n",
    "            num_columns.append(key)\n",
    "    num_columns.remove('S_2')  #Datetime; will have to remove this line once model becomes more sophisticated\n",
    "    \n",
    "    valtype_dict_cat = {}\n",
    "    for key in cat_vars:\n",
    "        if not key == 'customer_ID' and not key == 'S_2':\n",
    "            valtype_dict_cat[key] = [custom_mode, 'last']\n",
    "    \n",
    "    \n",
    "    print(cat_data0.columns)\n",
    "    print(valtype_dict_cat)\n",
    "    cat_data_grouped = cat_data0.groupby(by='customer_ID').aggregate(valtype_dict_cat)\n",
    "    \n",
    "    cat_data_grouped_dummies = dd.get_dummies(cat_data_grouped, drop_first=False, \n",
    "                               columns=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', \n",
    "                                        'D_64','D_66', 'D_68'])\n",
    "    \n",
    "    \n",
    "    column_sets = []\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'B_30_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'B_38_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_114_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_116_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_117_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_120_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_126_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_63_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_64_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_66_' in col])\n",
    "    column_sets.append([col for col in cat_data_grouped_dummies.columns if 'D_68_' in col])\n",
    "\n",
    "    \"\"\"\n",
    "    To ensure that data_onehot will have the desired shape when we use this function at test time.\n",
    "    https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data\n",
    "    \"\"\"\n",
    "    if not test_time:\n",
    "        # saves the onehot encoding columns we get from the training set so we can reshape the test data at test time\n",
    "        pickle.dump( cat_data_grouped_dummies.columns, open( \"./Train-Cat-Columns/train_cat_columns.p\", \"wb\" ) )\n",
    "        cat_data_almost_final = cat_data_grouped_dummies\n",
    "    else:\n",
    "        # reshaping onehot encoded test data to match the format of our onehot encoded training data\n",
    "        train_cat_columns = pickle.load(open('./Train-Cat-Columns/train_cat_columns.p', 'rb'))\n",
    "        cat_data_almost_final = cat_data_grouped_dummies.reindex(columns = train_cat_columns, fill_value=0)\n",
    "        \n",
    "    cat_data_final = cat_data_almost_final.apply(restore_nan_category, column_sets=column_sets, axis=1)\n",
    "    \n",
    "    print('DONE WITH ONLY DOING CAT STUFF')\n",
    "    \"\"\"\n",
    "    Now preparing the numerical variables\n",
    "    \"\"\"\n",
    "    dfl1 = dfl0[num_columns]\n",
    "    valtype_dict_num = {}\n",
    "    for key in dfl1.columns:\n",
    "        if not key == 'customer_ID' and not key == 'S_2':\n",
    "            if key in num_columns:\n",
    "                if key == 'P_2':\n",
    "                    # 'size' represents number of rows for given customer_ID; we only need to add this once\n",
    "                    valtype_dict_num[key] = ['mean', 'std', 'min', 'max', 'size', 'last']\n",
    "                else:\n",
    "                    valtype_dict_num[key] = ['mean', 'std', 'min', 'max', 'last']\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    dfl1_num_columns = dfl1\n",
    "    # Added the split_out=5 argument because I was getting worker killed errors from running out of RAM\n",
    "    dfl1_num_columns_grouped = dfl1_num_columns.groupby(by=\"customer_ID\").aggregate(valtype_dict_num, split_out=5)\n",
    "    print('finished aggregate on num data')\n",
    "    \"\"\"\n",
    "    Joining the numerical and categorical variables back together\n",
    "    \"\"\"\n",
    "    \n",
    "    final_df = dfl1_num_columns_grouped.merge(cat_data_final, how=\"left\", on=['customer_ID'])\n",
    "    print('finished merge')\n",
    "    \n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b87112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Wrapper function to train the model. Uses https://xgboost.readthedocs.io/en/stable/tutorials/dask.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "def train_xgboost(train_data_onehot, train_labels, n_boost_rounds, client, validate, X_valid=None, y_valid=None):\n",
    "\n",
    "#     dtrain = xgb.dask.DaskDMatrix(client, train_data_onehot, train_labels)\n",
    "    dtrain = xgb.DMatrix(train_data_onehot, train_labels)\n",
    "    \n",
    "    \n",
    "    if validate:\n",
    "#         dvalid = xgb.dask.DaskDMatrix(client, X_valid, y_valid)\n",
    "        dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "        evals=[(dtrain, \"train\"), (dvalid, 'valid')]\n",
    "        early_stopping_rounds = 1000\n",
    "    else:\n",
    "        evals=[(dtrain, \"train\")]\n",
    "        early_stopping_rounds = None\n",
    "        \n",
    "        \n",
    "\n",
    "    # Using Chris Deotte's model parameters from here: https://www.kaggle.com/code/cdeotte/xgboost-starter-0-793\n",
    "    \n",
    "    output = xgb.train(\n",
    "        {'max_depth':4, \n",
    "        'learning_rate':0.05, \n",
    "        'subsample':0.8,\n",
    "        'colsample_bytree':0.6, \n",
    "        'eval_metric':'logloss',\n",
    "        'objective':'binary:logistic'},\n",
    "        \n",
    "        dtrain,\n",
    "        num_boost_round=n_boost_rounds,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=early_stopping_rounds\n",
    "    )\n",
    "    \n",
    "#     output = xgb.dask.train(\n",
    "#         client,\n",
    "#         {'max_depth':4, \n",
    "#         'learning_rate':0.05, \n",
    "#         'subsample':0.8,\n",
    "#         'colsample_bytree':0.6, \n",
    "#         'eval_metric':'logloss',\n",
    "#         'objective':'binary:logistic'},\n",
    "        \n",
    "#         dtrain,\n",
    "#         num_boost_round=n_boost_rounds,\n",
    "#         evals=evals,\n",
    "#         early_stopping_rounds=early_stopping_rounds\n",
    "#     )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146d38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uncomment here and comment out the 'train_data == dd.read_csv' line below if \n",
    "testing with 10,000 line subset of training data.\n",
    "\"\"\"\n",
    "\n",
    "# train_data = pd.read_csv('./train_data.csv', nrows=10000)\n",
    "# train_data = dd.from_pandas(train_data, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e24b9f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing CAT stuff\n",
      "Index(['customer_ID', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
      "       'D_126', 'D_63', 'D_64', 'D_66', 'D_68'],\n",
      "      dtype='object')\n",
      "{'B_30': [<function custom_mode at 0x160eb8e50>, 'last'], 'B_38': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_114': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_116': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_117': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_120': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_126': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_63': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_64': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_66': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_68': [<function custom_mode at 0x160eb8e50>, 'last']}\n",
      "DONE WITH ONLY DOING CAT STUFF\n",
      "finished aggregate on num data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_16177/3830490681.py:125: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  final_df = dfl1_num_columns_grouped.merge(cat_data_final, how=\"left\", on=['customer_ID'])\n",
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_16177/3830490681.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_df = dfl1_num_columns_grouped.merge(cat_data_final, how=\"left\", on=['customer_ID'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/dask/dataframe/multi.py:525: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  return pd.merge(\n",
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.66265\tvalid-logloss:0.66266\n",
      "[1]\ttrain-logloss:0.63504\tvalid-logloss:0.63530\n",
      "[2]\ttrain-logloss:0.60904\tvalid-logloss:0.60950\n",
      "[3]\ttrain-logloss:0.58615\tvalid-logloss:0.58667\n",
      "[4]\ttrain-logloss:0.56482\tvalid-logloss:0.56553\n",
      "[5]\ttrain-logloss:0.54544\tvalid-logloss:0.54618\n",
      "[6]\ttrain-logloss:0.52669\tvalid-logloss:0.52768\n",
      "[7]\ttrain-logloss:0.50983\tvalid-logloss:0.51089\n",
      "[8]\ttrain-logloss:0.49402\tvalid-logloss:0.49495\n",
      "[9]\ttrain-logloss:0.47916\tvalid-logloss:0.48022\n",
      "[10]\ttrain-logloss:0.46573\tvalid-logloss:0.46689\n",
      "[11]\ttrain-logloss:0.45307\tvalid-logloss:0.45436\n",
      "[12]\ttrain-logloss:0.44110\tvalid-logloss:0.44247\n",
      "[13]\ttrain-logloss:0.42999\tvalid-logloss:0.43143\n",
      "[14]\ttrain-logloss:0.41975\tvalid-logloss:0.42139\n",
      "[15]\ttrain-logloss:0.40999\tvalid-logloss:0.41170\n",
      "[16]\ttrain-logloss:0.40092\tvalid-logloss:0.40265\n",
      "[17]\ttrain-logloss:0.39261\tvalid-logloss:0.39439\n",
      "[18]\ttrain-logloss:0.38451\tvalid-logloss:0.38641\n",
      "[19]\ttrain-logloss:0.37695\tvalid-logloss:0.37897\n",
      "[20]\ttrain-logloss:0.37005\tvalid-logloss:0.37208\n",
      "[21]\ttrain-logloss:0.36346\tvalid-logloss:0.36564\n",
      "[22]\ttrain-logloss:0.35718\tvalid-logloss:0.35936\n",
      "[23]\ttrain-logloss:0.35142\tvalid-logloss:0.35357\n",
      "[24]\ttrain-logloss:0.34582\tvalid-logloss:0.34806\n",
      "[25]\ttrain-logloss:0.34048\tvalid-logloss:0.34282\n",
      "[26]\ttrain-logloss:0.33560\tvalid-logloss:0.33802\n",
      "[27]\ttrain-logloss:0.33095\tvalid-logloss:0.33341\n",
      "[28]\ttrain-logloss:0.32647\tvalid-logloss:0.32894\n",
      "[29]\ttrain-logloss:0.32225\tvalid-logloss:0.32484\n",
      "[30]\ttrain-logloss:0.31825\tvalid-logloss:0.32086\n",
      "[31]\ttrain-logloss:0.31442\tvalid-logloss:0.31706\n",
      "[32]\ttrain-logloss:0.31089\tvalid-logloss:0.31350\n",
      "[33]\ttrain-logloss:0.30732\tvalid-logloss:0.31006\n",
      "[34]\ttrain-logloss:0.30411\tvalid-logloss:0.30687\n",
      "[35]\ttrain-logloss:0.30109\tvalid-logloss:0.30382\n",
      "[36]\ttrain-logloss:0.29812\tvalid-logloss:0.30090\n",
      "[37]\ttrain-logloss:0.29535\tvalid-logloss:0.29812\n",
      "[38]\ttrain-logloss:0.29272\tvalid-logloss:0.29553\n",
      "[39]\ttrain-logloss:0.29020\tvalid-logloss:0.29308\n",
      "[40]\ttrain-logloss:0.28777\tvalid-logloss:0.29071\n",
      "[41]\ttrain-logloss:0.28554\tvalid-logloss:0.28845\n",
      "[42]\ttrain-logloss:0.28340\tvalid-logloss:0.28636\n",
      "[43]\ttrain-logloss:0.28128\tvalid-logloss:0.28433\n",
      "[44]\ttrain-logloss:0.27930\tvalid-logloss:0.28236\n",
      "[45]\ttrain-logloss:0.27742\tvalid-logloss:0.28057\n",
      "[46]\ttrain-logloss:0.27570\tvalid-logloss:0.27883\n",
      "[47]\ttrain-logloss:0.27395\tvalid-logloss:0.27716\n",
      "[48]\ttrain-logloss:0.27222\tvalid-logloss:0.27545\n",
      "[49]\ttrain-logloss:0.27057\tvalid-logloss:0.27387\n",
      "[50]\ttrain-logloss:0.26900\tvalid-logloss:0.27235\n",
      "[51]\ttrain-logloss:0.26756\tvalid-logloss:0.27093\n",
      "[52]\ttrain-logloss:0.26619\tvalid-logloss:0.26958\n",
      "[53]\ttrain-logloss:0.26487\tvalid-logloss:0.26828\n",
      "[54]\ttrain-logloss:0.26363\tvalid-logloss:0.26711\n",
      "[55]\ttrain-logloss:0.26245\tvalid-logloss:0.26595\n",
      "[56]\ttrain-logloss:0.26129\tvalid-logloss:0.26484\n",
      "[57]\ttrain-logloss:0.26022\tvalid-logloss:0.26379\n",
      "[58]\ttrain-logloss:0.25914\tvalid-logloss:0.26275\n",
      "[59]\ttrain-logloss:0.25814\tvalid-logloss:0.26174\n",
      "[60]\ttrain-logloss:0.25709\tvalid-logloss:0.26074\n",
      "[61]\ttrain-logloss:0.25611\tvalid-logloss:0.25976\n",
      "[62]\ttrain-logloss:0.25517\tvalid-logloss:0.25886\n",
      "[63]\ttrain-logloss:0.25428\tvalid-logloss:0.25802\n",
      "[64]\ttrain-logloss:0.25350\tvalid-logloss:0.25725\n",
      "[65]\ttrain-logloss:0.25269\tvalid-logloss:0.25649\n",
      "[66]\ttrain-logloss:0.25190\tvalid-logloss:0.25578\n",
      "[67]\ttrain-logloss:0.25107\tvalid-logloss:0.25495\n",
      "[68]\ttrain-logloss:0.25031\tvalid-logloss:0.25423\n",
      "[69]\ttrain-logloss:0.24964\tvalid-logloss:0.25359\n",
      "[70]\ttrain-logloss:0.24897\tvalid-logloss:0.25296\n",
      "[71]\ttrain-logloss:0.24835\tvalid-logloss:0.25235\n",
      "[72]\ttrain-logloss:0.24771\tvalid-logloss:0.25171\n",
      "[73]\ttrain-logloss:0.24710\tvalid-logloss:0.25112\n",
      "[74]\ttrain-logloss:0.24650\tvalid-logloss:0.25058\n",
      "[75]\ttrain-logloss:0.24591\tvalid-logloss:0.25000\n",
      "[76]\ttrain-logloss:0.24532\tvalid-logloss:0.24943\n",
      "[77]\ttrain-logloss:0.24479\tvalid-logloss:0.24894\n",
      "[78]\ttrain-logloss:0.24426\tvalid-logloss:0.24845\n",
      "[79]\ttrain-logloss:0.24374\tvalid-logloss:0.24797\n",
      "[80]\ttrain-logloss:0.24324\tvalid-logloss:0.24750\n",
      "[81]\ttrain-logloss:0.24272\tvalid-logloss:0.24697\n",
      "[82]\ttrain-logloss:0.24224\tvalid-logloss:0.24652\n",
      "[83]\ttrain-logloss:0.24181\tvalid-logloss:0.24613\n",
      "[84]\ttrain-logloss:0.24134\tvalid-logloss:0.24572\n",
      "[85]\ttrain-logloss:0.24090\tvalid-logloss:0.24530\n",
      "[86]\ttrain-logloss:0.24048\tvalid-logloss:0.24493\n",
      "[87]\ttrain-logloss:0.24005\tvalid-logloss:0.24449\n",
      "[88]\ttrain-logloss:0.23957\tvalid-logloss:0.24404\n",
      "[89]\ttrain-logloss:0.23920\tvalid-logloss:0.24371\n",
      "[90]\ttrain-logloss:0.23882\tvalid-logloss:0.24337\n",
      "[91]\ttrain-logloss:0.23844\tvalid-logloss:0.24303\n",
      "[92]\ttrain-logloss:0.23808\tvalid-logloss:0.24268\n",
      "[93]\ttrain-logloss:0.23776\tvalid-logloss:0.24240\n",
      "[94]\ttrain-logloss:0.23742\tvalid-logloss:0.24210\n",
      "[95]\ttrain-logloss:0.23710\tvalid-logloss:0.24179\n",
      "[96]\ttrain-logloss:0.23681\tvalid-logloss:0.24152\n",
      "[97]\ttrain-logloss:0.23649\tvalid-logloss:0.24121\n",
      "[98]\ttrain-logloss:0.23617\tvalid-logloss:0.24090\n",
      "[99]\ttrain-logloss:0.23585\tvalid-logloss:0.24063\n",
      "[100]\ttrain-logloss:0.23553\tvalid-logloss:0.24031\n",
      "[101]\ttrain-logloss:0.23522\tvalid-logloss:0.24002\n",
      "[102]\ttrain-logloss:0.23493\tvalid-logloss:0.23977\n",
      "[103]\ttrain-logloss:0.23465\tvalid-logloss:0.23952\n",
      "[104]\ttrain-logloss:0.23438\tvalid-logloss:0.23928\n",
      "[105]\ttrain-logloss:0.23412\tvalid-logloss:0.23904\n",
      "[106]\ttrain-logloss:0.23387\tvalid-logloss:0.23883\n",
      "[107]\ttrain-logloss:0.23362\tvalid-logloss:0.23860\n",
      "[108]\ttrain-logloss:0.23339\tvalid-logloss:0.23840\n",
      "[109]\ttrain-logloss:0.23315\tvalid-logloss:0.23818\n",
      "[110]\ttrain-logloss:0.23289\tvalid-logloss:0.23792\n",
      "[111]\ttrain-logloss:0.23267\tvalid-logloss:0.23776\n",
      "[112]\ttrain-logloss:0.23245\tvalid-logloss:0.23758\n",
      "[113]\ttrain-logloss:0.23223\tvalid-logloss:0.23739\n",
      "[114]\ttrain-logloss:0.23200\tvalid-logloss:0.23721\n",
      "[115]\ttrain-logloss:0.23179\tvalid-logloss:0.23703\n",
      "[116]\ttrain-logloss:0.23160\tvalid-logloss:0.23684\n",
      "[117]\ttrain-logloss:0.23139\tvalid-logloss:0.23667\n",
      "[118]\ttrain-logloss:0.23118\tvalid-logloss:0.23647\n",
      "[119]\ttrain-logloss:0.23099\tvalid-logloss:0.23630\n",
      "[120]\ttrain-logloss:0.23078\tvalid-logloss:0.23613\n",
      "[121]\ttrain-logloss:0.23059\tvalid-logloss:0.23594\n",
      "[122]\ttrain-logloss:0.23041\tvalid-logloss:0.23578\n",
      "[123]\ttrain-logloss:0.23024\tvalid-logloss:0.23563\n",
      "[124]\ttrain-logloss:0.23005\tvalid-logloss:0.23548\n",
      "[125]\ttrain-logloss:0.22988\tvalid-logloss:0.23534\n",
      "[126]\ttrain-logloss:0.22969\tvalid-logloss:0.23518\n",
      "[127]\ttrain-logloss:0.22952\tvalid-logloss:0.23505\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m y_full_train \u001b[38;5;241m=\u001b[39m train_data_onehot_and_labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mUse first line below when validating; use second line when training on the full training dataset\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_xgboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mtrain_xgboost\u001b[0;34m(train_data_onehot, train_labels, n_boost_rounds, client, validate, X_valid, y_valid)\u001b[0m\n\u001b[1;32m     26\u001b[0m         early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Using Chris Deotte's model parameters from here: https://www.kaggle.com/code/cdeotte/xgboost-starter-0-793\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubsample\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolsample_bytree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_metric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobjective\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary:logistic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_boost_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#     output = xgb.dask.train(\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#         client,\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#         {'max_depth':4, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#         early_stopping_rounds=early_stopping_rounds\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, evals\u001b[38;5;241m=\u001b[39m(), obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m           maximize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, evals_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m           verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xgb_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124;03m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Booster : a trained booster model\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     bst \u001b[38;5;241m=\u001b[39m \u001b[43m_train_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bst\n",
      "File \u001b[0;32m~/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/training.py:81\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1680\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Processing the data and training the model\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Uncomment below if using the full training dataset. Additionally, experiment with increasing blocksize above 25e6.\n",
    "# train_data = dd.read_csv('./train_data.csv') #, blocksize=100e6)\n",
    "# train_data = dd.read_parquet('./Train-Parquet')\n",
    "\"\"\"\n",
    "Using Raddar's lightweight version of the train and test datasets \n",
    "https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format?select=test.parquet\n",
    "\"\"\"\n",
    "\n",
    "train_data = pd.read_parquet('./Train-Parquet-Lightweight')\n",
    "\n",
    "train_data2 = train_data\n",
    "train_labels = pd.read_csv('./train_labels.csv')\n",
    "\n",
    "train_data_prepared = prepare_data(train_data2, False)\n",
    "\n",
    "\n",
    "# We do the merge below in order to ensure that the partitions of the dask dataframes line up\n",
    "train_data_onehot_and_labels = dd.merge(train_data_prepared, train_labels, on=['customer_ID'])\n",
    "\n",
    "# Split into train and validation sets\n",
    "# train_final, valid_final = train_data_onehot_and_labels.random_split([0.8, 0.2])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_data_onehot_and_labels.drop(columns=['target','customer_ID']), \n",
    "    train_data_onehot_and_labels['target'], test_size=0.20)\n",
    "\n",
    "# X_train = train_final.drop(columns=['target','customer_ID'])\n",
    "# y_train = train_final['target']\n",
    "\n",
    "# X_valid = valid_final.drop(columns=['target','customer_ID'])\n",
    "# y_valid = valid_final['target']\n",
    "\n",
    "X_full_train = train_data_onehot_and_labels.drop(columns=['target','customer_ID'])\n",
    "y_full_train = train_data_onehot_and_labels['target']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use first line below when validating; use second line when training on the full training dataset\n",
    "\"\"\"\n",
    "output = train_xgboost(X_train, y_train, 5000, client, True, X_valid, y_valid)\n",
    "# output = train_xgboost(X_full_train, y_full_train, 1947, client, validate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ba9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37caf9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump( output, open( \"./Models/mean_std_min_max_last_dart_4.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe03402",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "1\n",
      "Doing CAT stuff\n",
      "Index(['customer_ID', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
      "       'D_126', 'D_63', 'D_64', 'D_66', 'D_68'],\n",
      "      dtype='object')\n",
      "{'B_30': [<function custom_mode at 0x15aa853f0>, 'last'], 'B_38': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_114': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_116': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_117': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_120': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_126': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_63': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_64': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_66': [<function custom_mode at 0x15aa853f0>, 'last'], 'D_68': [<function custom_mode at 0x15aa853f0>, 'last']}\n",
      "DONE WITH ONLY DOING CAT STUFF\n",
      "finished aggregate on num data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/dask/dataframe/multi.py:399: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  meta = left._meta_nonempty.merge(right._meta_nonempty, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished merge\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ToDo: Now we start writing code to load the model trained above, and then ultimately output a submission .csv file\n",
    "containing our predictions over the test dataset.\n",
    "\"\"\"\n",
    "import dask.dataframe as dd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_path = './Models/mean_std_min_max_last_dart_4.p' \n",
    "file = open(model_path, 'rb')\n",
    "model_output = pickle.load(file)\n",
    "booster = model_output[\"booster\"]\n",
    "print(booster.best_iteration)\n",
    "best_model = booster[: booster.best_iteration]\n",
    "\n",
    "# test_data = dd.read_csv('./test_data.csv')#, blocksize=100e6)\n",
    "test_data = dd.read_parquet('./Test-Parquet')#, split_row_groups=1000)\n",
    "# customers = pd.read_parquet('./Test-Parquet', columns=['customer_ID']).to_numpy()\n",
    "print(1)\n",
    "prepared_test_data = prepare_data(test_data, True)\n",
    "customers = prepared_test_data.index\n",
    "print(2)\n",
    "\n",
    "dtest = xgb.dask.DaskDMatrix(client, prepared_test_data)\n",
    "prediction = xgb.dask.predict(client, best_model, dtest)\n",
    "print(3)\n",
    "\n",
    "output_df = pd.DataFrame({'customer_ID' : list(customers), 'prediction' : np.array(prediction)})\n",
    "print(4)\n",
    "output_df.to_csv('./Outputs/submission8.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ad390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing CAT stuff\n",
      "Index(['customer_ID', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
      "       'D_126', 'D_63', 'D_64', 'D_66', 'D_68'],\n",
      "      dtype='object')\n",
      "{'B_30': [<function custom_mode at 0x160eb8e50>, 'last'], 'B_38': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_114': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_116': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_117': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_120': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_126': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_63': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_64': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_66': [<function custom_mode at 0x160eb8e50>, 'last'], 'D_68': [<function custom_mode at 0x160eb8e50>, 'last']}\n",
      "DONE WITH ONLY DOING CAT STUFF\n",
      "finished aggregate on num data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_16177/3830490681.py:125: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (2 levels on the left, 1 on the right)\n",
      "  final_df = dfl1_num_columns_grouped.merge(cat_data_final, how=\"left\", on=['customer_ID'])\n",
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_16177/3830490681.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_df = dfl1_num_columns_grouped.merge(cat_data_final, how=\"left\", on=['customer_ID'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/dask/dataframe/multi.py:525: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  return pd.merge(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsample |   gamma   | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:02:54] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:15:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/johnblackwelder/miniforge3/envs/amex_env/lib/python3.10/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:29:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hyperparameter tuning. Code adapted from:\n",
    "https://aiinpractice.com/xgboost-hyperparameter-tuning-with-bayesian-optimization/\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "global train_final\n",
    "\n",
    "pbounds = {\n",
    "    'learning_rate': (0.01, 1.0),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3,10),\n",
    "    'subsample': (1.0, 1.0),  # Change for big datasets\n",
    "    'colsample': (1.0, 1.0),  # Change for datasets with lots of features\n",
    "    'gamma': (0, 5)}\n",
    "\n",
    "def xgboost_hyper_param(learning_rate,\n",
    "                        n_estimators,\n",
    "                        max_depth,\n",
    "                        subsample,\n",
    "                        colsample,\n",
    "                        gamma):\n",
    "\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        gamma=gamma)\n",
    "#     X_train = train_final.drop(columns=['target','customer_ID'])\n",
    "#     y_train = train_final['target']\n",
    "    return np.mean(cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc'))\n",
    "\n",
    "train_data = pd.read_parquet('./Train-Parquet-Lightweight')\n",
    "train_data2 = train_data\n",
    "train_labels = pd.read_csv('./train_labels.csv')\n",
    "train_data_prepared = prepare_data(train_data2, False)\n",
    "train_final = dd.merge(train_data_prepared, train_labels, on=['customer_ID'])\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgboost_hyper_param,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100e25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amex_env]",
   "language": "python",
   "name": "conda-env-amex_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
